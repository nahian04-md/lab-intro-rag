{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsnCPbdkxYZd"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1 style=\"color: #FF6347;\">Self-Guided Lab: Retrieval-Augmented Generation (RAGs)</h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZp4BQAVxYZj"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ3FsdzRveTBrenMxM3VnbDMwaTJxN2NnZm50aGFibXk1NzNnY2Q0MCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/LR5ZBwZHv02lmpVoEU/giphy.gif\" alt=\"NLP Gif\" style=\"width: 300px; height: 150px; object-fit: cover; object-position: center;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gizk6HCYxYZo"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Data Storage & Retrieval</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW5UOI8ZxYZp"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">PyPDFLoader</h2>\n",
        "\n",
        "`PyPDFLoader` is a lightweight Python library designed to streamline the process of loading and parsing PDF documents for text processing tasks. It is particularly useful in Retrieval-Augmented Generation workflows where text extraction from PDFs is required.\n",
        "\n",
        "- **What Does PyPDFLoader Do?**\n",
        "  - Extracts text from PDF files, retaining formatting and layout.\n",
        "  - Simplifies the preprocessing of document-based datasets.\n",
        "  - Supports efficient and scalable loading of large PDF collections.\n",
        "\n",
        "- **Key Features:**\n",
        "  - Compatible with popular NLP libraries and frameworks.\n",
        "  - Handles multi-page PDFs and embedded images (e.g., OCR-compatible setups).\n",
        "  - Provides flexible configurations for structured text extraction.\n",
        "\n",
        "- **Use Cases:**\n",
        "  - Preparing PDF documents for retrieval-based systems in RAGs.\n",
        "  - Automating the text extraction pipeline for document analysis.\n",
        "  - Creating datasets from academic papers, technical manuals, and reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\nahia\\anaconda3\\lib\\site-packages (1.0.8)\n",
            "Requirement already satisfied: langchain_community in c:\\users\\nahia\\anaconda3\\lib\\site-packages (0.4.1)\n",
            "Requirement already satisfied: pypdf in c:\\users\\nahia\\anaconda3\\lib\\site-packages (6.3.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.45)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (24.2)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (2.1)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.7.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.7)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain_community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain_community) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain_community) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain_community) (2.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: termcolor in c:\\users\\nahia\\anaconda3\\lib\\site-packages (3.2.0)\n",
            "Requirement already satisfied: langchain_openai in c:\\users\\nahia\\anaconda3\\lib\\site-packages (1.0.3)\n",
            "Requirement already satisfied: langchain-huggingface in c:\\users\\nahia\\anaconda3\\lib\\site-packages (1.0.1)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\nahia\\anaconda3\\lib\\site-packages (5.1.2)\n",
            "Requirement already satisfied: chromadb in c:\\users\\nahia\\anaconda3\\lib\\site-packages (1.3.5)\n",
            "Requirement already satisfied: langchain_chroma in c:\\users\\nahia\\anaconda3\\lib\\site-packages (1.0.0)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\nahia\\anaconda3\\lib\\site-packages (0.12.0)\n",
            "Requirement already satisfied: openai in c:\\users\\nahia\\anaconda3\\lib\\site-packages (2.8.1)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\nahia\\anaconda3\\lib\\site-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai) (4.7.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: certifi in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (0.4.45)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (24.2)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.2->langchain_openai) (2.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain_openai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: Pillow in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: build>=1.0.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: pyproject_hooks in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
            "Requirement already satisfied: coloredlogs in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
            "Requirement already satisfied: sympy in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain langchain_community pypdf\n",
        "%pip install termcolor langchain_openai langchain-huggingface sentence-transformers chromadb langchain_chroma tiktoken openai python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-text-splitters in c:\\users\\nahia\\anaconda3\\lib\\site-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-text-splitters) (1.0.7)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.45)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.10.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.23.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.7.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.7)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.27.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRS44B2XxYZs",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Loading the Documents</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cuREtJRixYZt"
      },
      "outputs": [],
      "source": [
        "# File path for the document\n",
        "\n",
        "file_path = r\"C:\\Users\\nahia\\OneDrive\\Escritorio\\Ironhack_Bootcamp\\Woche_Sieben\\lab-intro-rag\\ai-for-everyone.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz_8SOLxxYZt"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Documents into pages</h3>\n",
        "\n",
        "The `PyPDFLoader` library allows efficient loading and splitting of PDF documents into smaller, manageable parts for NLP tasks.\n",
        "\n",
        "This functionality is particularly useful in workflows requiring granular text processing, such as Retrieval-Augmented Generation (RAG).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_b5Z_45UxYZu",
        "outputId": "a600d69f-14fe-4492-f236-97261d6ff36c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "297"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and split the document\n",
        "loader = PyPDFLoader(file_path)\n",
        "pages = loader.load_and_split()\n",
        "len(pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt50NRQaxYZv"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Pages into Chunks</h3>\n",
        "\n",
        "\n",
        "####  RecursiveCharacterTextSplitter in LangChain\n",
        "\n",
        "The `RecursiveCharacterTextSplitter` is the **recommended splitter** in LangChain when you want to break down long documents into smaller, semantically meaningful chunks — especially useful in **RAG pipelines**, where clean context chunks lead to better LLM responses.\n",
        "\n",
        "####  Parameters\n",
        "\n",
        "| Parameter       | Description                                                                 |\n",
        "|-----------------|-----------------------------------------------------------------------------|\n",
        "| `chunk_size`    | The **maximum number of characters** allowed in a chunk (e.g., `1000`).     |\n",
        "| `chunk_overlap` | The number of **overlapping characters** between consecutive chunks (e.g., `200`). This helps preserve context continuity. |\n",
        "\n",
        "####  How it works\n",
        "`RecursiveCharacterTextSplitter` attempts to split the text **intelligently**, trying the following separators in order:\n",
        "1. Paragraphs (`\"\\n\\n\"`)\n",
        "2. Lines (`\"\\n\"`)\n",
        "3. Sentences or words (`\" \"`)\n",
        "4. Individual characters (as a last resort)\n",
        "\n",
        "This makes it ideal for handling **natural language documents**, such as PDFs, articles, or long reports, without breaking sentences or paragraphs in awkward ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1096"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Alternative: CharacterTextSplitter\n",
        "\n",
        "`CharacterTextSplitter` is a simpler splitter that breaks text into chunks based **purely on character count**, without trying to preserve any natural language structure.\n",
        "\n",
        "##### Example:\n",
        "```python\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "````\n",
        "\n",
        "This method is faster and more predictable but may split text in the middle of a sentence or paragraph, which can hurt performance in downstream tasks like retrieval or QA.\n",
        "\n",
        "---\n",
        "\n",
        "#### Comparison Table\n",
        "\n",
        "| Feature                        | RecursiveCharacterTextSplitter | CharacterTextSplitter     |\n",
        "| ------------------------------ | ------------------------------ | ------------------------- |\n",
        "| Structure-aware splitting      |  Yes                          |  No                      |\n",
        "| Preserves sentence/paragraphs  |  Yes                          |  No                      |\n",
        "| Risk of splitting mid-sentence |  Minimal                     |  High                   |\n",
        "| Ideal for RAG/document QA      |  Highly recommended           |  Only if structured text |\n",
        "| Performance speed              |  Slightly slower             |  Faster                  |\n",
        "\n",
        "---\n",
        "\n",
        "#### Recommendation\n",
        "\n",
        "Use `RecursiveCharacterTextSplitter` for most real-world document processing tasks, especially when building RAG pipelines or working with structured natural language content like PDFs or articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Choosing Chunk Size in RAG\n",
        "\n",
        "### Best Practices for Chunk Size in RAG\n",
        "\n",
        "| Factor                      | Recommendation                                                                                                                                                                                          |\n",
        "| ---------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **LLM context limit**       | Choose a chunk size that lets you retrieve multiple chunks **without exceeding the model’s token limit**. For example, GPT-4o supports 128k tokens, but with GPT-3.5 (16k) or GPT-4 (32k), keep it modest. |\n",
        "| **Chunk size (in characters)** | Typically: **500–1,000 characters** per chunk → ~75–200 tokens. This fits well for retrieval + prompt without context overflow.                                                                           |\n",
        "| **Chunk size (in tokens)**  | If using token-based splitter (e.g. `TokenTextSplitter`): aim for **100–300 tokens** per chunk.                                                                                                            |\n",
        "| **Chunk overlap**           | Use **overlap of 10–30%** (e.g., 100–300 characters or ~50 tokens) to preserve context across chunk boundaries and avoid cutting off important ideas mid-sentence.                                        |\n",
        "| **Document structure**      | Use **`RecursiveCharacterTextSplitter`** to preserve semantic boundaries (paragraphs, sentences) instead of arbitrary cuts.                                                                                |\n",
        "| **Task type**               | For **question answering**, smaller chunks (~500–800 chars) reduce noise.<br>For **summarization**, slightly larger chunks (~1000–1500) are OK.                                                          |\n",
        "| **Embedding model**         | Some models (e.g., `text-embedding-3-large`) can handle long input. But still, smaller chunks give **finer-grained retrieval**, which improves relevance.                                                  |\n",
        "| **Query type**              | If users ask **very specific questions**, small focused chunks are better. For broader queries, bigger chunks might help.                                                                                  |\n",
        "\n",
        "\n",
        "### Rule of Thumb\n",
        "\n",
        "| Use Case                 | Chunk Size      | Overlap |\n",
        "| ------------------------| --------------- | ------- |\n",
        "| Factual Q&A              | 500–800 chars   | 100–200 |\n",
        "| Summarization            | 1000–1500 chars | 200–300 |\n",
        "| Technical documents      | 400–700 chars   | 100–200 |\n",
        "| Long reports/books       | 800–1200 chars  | 200–300 |\n",
        "| Small LLMs (≤16k tokens) | ≤800 chars      | 100–200 |\n",
        "\n",
        "\n",
        "### Avoid\n",
        "\n",
        "- Chunks >2000 characters: risks context overflow.\n",
        "- No overlap: may lose key information between chunks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg15RjVPxYZw"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Embeddings</h2>\n",
        "\n",
        "Embeddings transform text into dense vector representations, capturing semantic meaning and contextual relationships. They are essential for efficient document retrieval and similarity analysis.\n",
        "\n",
        "- **What are OpenAI Embeddings?**\n",
        "  - Pre-trained embeddings like `text-embedding-3-large` generate high-quality vector representations for text.\n",
        "  - Encapsulate semantic relationships in the text, enabling robust NLP applications.\n",
        "\n",
        "- **Key Features of `text-embedding-3-large`:**\n",
        "  - Large-scale embedding model optimized for accuracy and versatility.\n",
        "  - Handles diverse NLP tasks, including retrieval, classification, and clustering.\n",
        "  - Ideal for applications with high-performance requirements.\n",
        "\n",
        "- **Benefits:**\n",
        "  - Reduces the need for extensive custom training.\n",
        "  - Provides state-of-the-art performance in retrieval-augmented systems.\n",
        "  - Compatible with RAGs to create powerful context-aware models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-openai in c:\\users\\nahia\\anaconda3\\lib\\site-packages (1.0.3)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-openai) (2.8.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.4.45)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.10.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (4.7.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.7)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain-openai\n",
        "from langchain_openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv, find_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\nahia\\\\OneDrive\\\\Escritorio\\\\Ironhack_Bootcamp\\\\Woche_Sieben\\\\lab-intro-rag\\\\your-code\\\\.env'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_WRIo3_0xYZx",
        "outputId": "78bfbbf3-9d25-4e31-bdbc-3e932e6bbfec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MNZfTng5xYZz",
        "outputId": "db1a7c85-ef9f-447e-92cd-9d097e959847"
      },
      "outputs": [],
      "source": [
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsSA7RKvxYZz"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChromaDB</h2>\n",
        "\n",
        "ChromaDB is a versatile vector database designed for efficiently storing and retrieving embeddings. It integrates seamlessly with embedding models to enable high-performance similarity search and context-based retrieval.\n",
        "\n",
        "### Workflow Overview:\n",
        "- **Step 1:** Generate embeddings using a pre-trained model (e.g., OpenAI's `text-embedding-3-large`).\n",
        "- **Step 2:** Store the embeddings in ChromaDB for efficient retrieval and similarity calculations.\n",
        "- **Step 3:** Use the stored embeddings to perform searches, matching, or context-based retrieval.\n",
        "\n",
        "### Key Features of ChromaDB:\n",
        "- **Scalability:** Handles large-scale datasets with optimized indexing and search capabilities.\n",
        "- **Speed:** Provides fast and accurate retrieval of embeddings for real-time applications.\n",
        "- **Integration:** Supports integration with popular frameworks and libraries for embedding generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain<0.1\n",
            "  Using cached langchain-0.0.354-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain<0.1) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain<0.1) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain<0.1) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain<0.1) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain<0.1) (1.33)\n",
            "Collecting langchain-community<0.1,>=0.0.8 (from langchain<0.1)\n",
            "  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting langchain-core<0.2,>=0.1.5 (from langchain<0.1)\n",
            "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.77 (from langchain<0.1)\n",
            "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting numpy<2,>=1 (from langchain<0.1)\n",
            "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [21 lines of output]\n",
            "      + c:\\Users\\nahia\\anaconda3\\python.exe C:\\Users\\nahia\\AppData\\Local\\Temp\\pip-install-hcdpuae4\\numpy_71322e8313354d168f87bc4da0ad0dd7\\vendored-meson\\meson\\meson.py setup C:\\Users\\nahia\\AppData\\Local\\Temp\\pip-install-hcdpuae4\\numpy_71322e8313354d168f87bc4da0ad0dd7 C:\\Users\\nahia\\AppData\\Local\\Temp\\pip-install-hcdpuae4\\numpy_71322e8313354d168f87bc4da0ad0dd7\\.mesonpy-xl306lmu -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\nahia\\AppData\\Local\\Temp\\pip-install-hcdpuae4\\numpy_71322e8313354d168f87bc4da0ad0dd7\\.mesonpy-xl306lmu\\meson-python-native-file.ini\n",
            "      The Meson build system\n",
            "      Version: 1.2.99\n",
            "      Source dir: C:\\Users\\nahia\\AppData\\Local\\Temp\\pip-install-hcdpuae4\\numpy_71322e8313354d168f87bc4da0ad0dd7\n",
            "      Build dir: C:\\Users\\nahia\\AppData\\Local\\Temp\\pip-install-hcdpuae4\\numpy_71322e8313354d168f87bc4da0ad0dd7\\.mesonpy-xl306lmu\n",
            "      Build type: native build\n",
            "      Project name: NumPy\n",
            "      Project version: 1.26.4\n",
            "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
            "      \n",
            "      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
            "      The following exception(s) were encountered:\n",
            "      Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      \n",
            "      A full log can be found at C:\\Users\\nahia\\AppData\\Local\\Temp\\pip-install-hcdpuae4\\numpy_71322e8313354d168f87bc4da0ad0dd7\\.mesonpy-xl306lmu\\meson-logs\\meson-log.txt\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        }
      ],
      "source": [
        "pip install \"langchain<0.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0.8\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-community in c:\\users\\nahia\\anaconda3\\lib\\site-packages (0.4.1)\n",
            "Requirement already satisfied: chromadb in c:\\users\\nahia\\anaconda3\\lib\\site-packages (1.3.5)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (1.0.7)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.39)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (3.11.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-community) (2.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (2.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.7.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (3.7)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: build>=1.0.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: pyproject_hooks in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
            "Requirement already satisfied: coloredlogs in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
            "Requirement already satisfied: sympy in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nahia\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB created with document embeddings.\n"
          ]
        }
      ],
      "source": [
        "db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db_lesson\")\n",
        "print(\"ChromaDB created with document embeddings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VkjHR-RkxYZ0",
        "outputId": "bc11bda9-f283-457a-f584-5a06b95c4dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB created with document embeddings.\n"
          ]
        }
      ],
      "source": [
        "db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db_LAB\")\n",
        "print(\"ChromaDB created with document embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27OdN1IVxYZ1"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Retrieving Documents</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice1: Write a user question that someone might ask about your book’s topic or content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "XiLv-TfrxYZ1"
      },
      "outputs": [],
      "source": [
        "user_question = \"how civil liberties can be compromised by AI?\" # User question\n",
        "retrieved_docs = db.similarity_search(user_question, k=10) # k is the number of documents to retrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qgWsh50JxYZ1",
        "outputId": "c8640c5d-5955-471f-fdd2-37096f5f68c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1:\n",
            "erate the concept of AI to alleviate some source of injustice and proceed to \n",
            "develop a technology that may be useful for a specific purpose, but does not \n",
            "have the backing of a significant entity to ensure that AI products are chosen to \n",
            "improve quality of life for the most vulnerable through a process of consulta -\n",
            "tion and oversight.\n",
            "Non-profits and other organisations dedicated to the alleviation of human \n",
            "suffering and improving justice are traditionally staffed by less technical per -\n",
            "sons, such as those with legal training rather than technical training in software\n",
            "Document 2:\n",
            "lved are making sincere efforts \n",
            "to be fair (which they often aren’t) (Eubanks 2018). The data demands of AI \n",
            "mean that the pattern of having to trade private personal information for ser -\n",
            "vices will become even more invasive. The optimisations of AI act as an inverse \n",
            "intersectionality, applying additional downward pressure on existing fissures in \n",
            "the social fabric. Like Eubanks, we should be asking what specific forms these \n",
            "fractures will take, and how to recognise them. One marker will be the emer -\n",
            "gence of machinic moralism. The more that AI is seen as a solution to austerity, \n",
            "the more its classifications and rankings will be enrolled in the rationing of \n",
            "goods and the assigning of sanctions. AI will be put in the position of decid -\n",
            "Document 3:\n",
            "ract principles of ethics and fair -\n",
            "ness. Using international human rights as a frame in relation to the governance \n",
            "of AI details the specificity of potential harms by linking them to particular \n",
            "rights, such as the right to freedom of association or the right to a fair trial, that \n",
            "can apply to different parts of social life (HRBDT 2020). These assertions of \n",
            "rights can help inform impact assessments, for example, when new AI systems \n",
            "are being developed or deployed (Jørgensen et al. 2019; Jansen 2020). By rely -\n",
            "ing on universal terms of reference, a human rights framework is also effective \n",
            "for advocacy as an internationally recognised agreement, however much this \n",
            "may not play out in practice. A recent court case brought forward by NGOs in\n"
          ]
        }
      ],
      "source": [
        "# Display top results\n",
        "for i, doc in enumerate(retrieved_docs[:3]): # Display top 3 results\n",
        "    print(f\"Document {i+1}:\\n{doc.page_content[200:10000]}\") # Display content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuGK8gL6xYZ1"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Preparing Content for GenAI</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2iB3lZqHxYZ2"
      },
      "outputs": [],
      "source": [
        "def _get_document_prompt(docs):\n",
        "    prompt = \"\\n\"\n",
        "    for doc in docs:\n",
        "        prompt += \"\\nContent:\\n\"\n",
        "        prompt += doc.page_content + \"\\n\\n\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2okzmuADxYZ2",
        "outputId": "0aa6cdca-188d-40e0-f5b4-8888d3549ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context formatted for GPT model.\n"
          ]
        }
      ],
      "source": [
        "# Generate a formatted context from the retrieved documents\n",
        "formatted_context = _get_document_prompt(retrieved_docs)\n",
        "print(\"Context formatted for GPT model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Content:\n",
            "never satisfy rights-based or duty-based obligations to other humans such as \n",
            "protection from persistent surveillance. \n",
            "However, AI for social good is ad hoc. That is to say, private individuals gen-\n",
            "erate the concept of AI to alleviate some source of injustice and proceed to \n",
            "develop a technology that may be useful for a specific purpose, but does not \n",
            "have the backing of a significant entity to ensure that AI products are chosen to \n",
            "improve quality of life for the most vulnerable th\n"
          ]
        }
      ],
      "source": [
        "print(formatted_context[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzIczQNTxYZ2"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChatBot Architecture</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice2: Write a prompt that is relevant and tailored to the content and style of your book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt constructed.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "\n",
        "## SYSTEM ROLE\n",
        "You are a knowledgeable and factual chatbot designed to assist with technical questions about **AI Usage**, specifically focusing on **Critical Perspectives**.\n",
        "Your answers must be based exclusively on provided content from technical books provided.\n",
        "\n",
        "## USER QUESTION\n",
        "The user has asked:\n",
        "\"{user_question}\"\n",
        "\n",
        "## CONTEXT\n",
        "Here is the relevant content from the technical books:\n",
        "'''\n",
        "{formatted_context}\n",
        "'''\n",
        "\n",
        "## GUIDELINES\n",
        "1. **Accuracy**:\n",
        "   - Only use the content in the `CONTEXT` section to answer.\n",
        "   - If the answer cannot be found, explicitly state: \"The provided context does not contain this information.\"\n",
        "   - Start explain AI  contribution to the society and civil liberties\n",
        "   - Follow by current correlation between AI and civil liberties\n",
        "   - Lastly explain AI legislation with civil liberties perspective.\n",
        "\n",
        "2. **Transparency**:\n",
        "   - Reference the book's name and page numbers when providing information.\n",
        "   - Do not speculate or provide opinions.\n",
        "\n",
        "3. **Clarity**:\n",
        "   - Use simple, professional, and concise language.\n",
        "   - Format your response in Markdown for readability.\n",
        "\n",
        "## TASK\n",
        "1. Answer the user's question **directly** if possible.\n",
        "2. Point the user to relevant parts of the documentation.\n",
        "3. Provide the response in the following format:\n",
        "\n",
        "## RESPONSE FORMAT\n",
        "'''\n",
        "# [Brief Title of the Answer]\n",
        "[Answer in simple, clear text.]\n",
        "\n",
        "**Source**:\n",
        "• [Book Title], Page(s): [...]\n",
        "'''\n",
        "\"\"\"\n",
        "print(\"Prompt constructed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "0mjkQJ_ZxYZ3"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice3: Tune parameters like temperature, and penalties to control how creative, focused, or varied the model's responses are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ylypRWRlxYZ4"
      },
      "outputs": [],
      "source": [
        "# Set up GPT client and parameters\n",
        "client = openai.OpenAI()\n",
        "model_params = {\n",
        "    'model': 'gpt-4o',\n",
        "    'temperature': 0.7,  # Increase creativity\n",
        "    'max_tokens':5000 ,  # Allow for longer responses\n",
        "    'top_p': 0.9,        # Use nucleus sampling\n",
        "    'frequency_penalty':0.4 ,  # Reduce repetition\n",
        "    'presence_penalty': 0.6   # Encourage new topics\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8e942xDxYZ4"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Response</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "4eXZO4pIxYZ4"
      },
      "outputs": [],
      "source": [
        "messages = [{'role': 'user', 'content': prompt}]\n",
        "completion = client.chat.completions.create(messages=messages, **model_params, timeout=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "## SYSTEM ROLE\n",
            "You are a knowledgeable and factual chatbot designed to assist with technical questions about **AI Usage**, specifically focusing on **Critical Perspectives**.\n",
            "Your answers must be based exclusively on provided content from technical books provided.\n",
            "\n",
            "## USER QUESTION\n",
            "The user has asked:\n",
            "\"how civil liberties can be compromised by AI?\"\n",
            "\n",
            "## CONTEXT\n",
            "Here is the relevant content from the technical books:\n",
            "'''\n",
            "\n",
            "\n",
            "Content:\n",
            "never satisfy rights-based or duty-based obligations to other humans such as \n",
            "protection from persistent surveillance. \n",
            "However, AI for social good is ad hoc. That is to say, private individuals gen-\n",
            "erate the concept of AI to alleviate some source of injustice and proceed to \n",
            "develop a technology that may be useful for a specific purpose, but does not \n",
            "have the backing of a significant entity to ensure that AI products are chosen to \n",
            "improve quality of life for the most vulnerable through a process of consulta -\n",
            "tion and oversight.\n",
            "Non-profits and other organisations dedicated to the alleviation of human \n",
            "suffering and improving justice are traditionally staffed by less technical per -\n",
            "sons, such as those with legal training rather than technical training in software\n",
            "\n",
            "\n",
            "Content:\n",
            "to pass for social class as well as race; how it seems to always be the poorest \n",
            "and most marginalised who bear the brunt of collateral damage from algorith-\n",
            "mic systems even when the bureaucrats involved are making sincere efforts \n",
            "to be fair (which they often aren’t) (Eubanks 2018). The data demands of AI \n",
            "mean that the pattern of having to trade private personal information for ser -\n",
            "vices will become even more invasive. The optimisations of AI act as an inverse \n",
            "intersectionality, applying additional downward pressure on existing fissures in \n",
            "the social fabric. Like Eubanks, we should be asking what specific forms these \n",
            "fractures will take, and how to recognise them. One marker will be the emer -\n",
            "gence of machinic moralism. The more that AI is seen as a solution to austerity, \n",
            "the more its classifications and rankings will be enrolled in the rationing of \n",
            "goods and the assigning of sanctions. AI will be put in the position of decid -\n",
            "\n",
            "\n",
            "Content:\n",
            "discussions. Drawing on human rights legislation in AI governance debates \n",
            "goes beyond issues of privacy and the protection of personal data whilst pro -\n",
            "viding a sturdier point of reference than abstract principles of ethics and fair -\n",
            "ness. Using international human rights as a frame in relation to the governance \n",
            "of AI details the specificity of potential harms by linking them to particular \n",
            "rights, such as the right to freedom of association or the right to a fair trial, that \n",
            "can apply to different parts of social life (HRBDT 2020). These assertions of \n",
            "rights can help inform impact assessments, for example, when new AI systems \n",
            "are being developed or deployed (Jørgensen et al. 2019; Jansen 2020). By rely -\n",
            "ing on universal terms of reference, a human rights framework is also effective \n",
            "for advocacy as an internationally recognised agreement, however much this \n",
            "may not play out in practice. A recent court case brought forward by NGOs in\n",
            "\n",
            "\n",
            "Content:\n",
            "and manipulation.\n",
            "Using the example of China, we will provide an illustration as to how far the \n",
            "possibilities of artificial intelligence reach with respect to control and super -\n",
            "vision. AI is largely embraced by the Chinese government, which uses it to \n",
            "track and monitor its citizens and inhabitants. For each individual, the Chinese  \n",
            "government calculates a so-called ‘social credit score’ based on (big) data com-\n",
            "ing from various different sources such as health and tax records, social media \n",
            "activity, purchasing behaviour, criminal records and so forth. The system also \n",
            "uses facial recognition and images of the 200 million surveillance cameras \n",
            "mounted across the country for data collection and respective score calcula -\n",
            "tion. Good behaviour such as volunteering at an orphanage leads to higher \n",
            "scores; bad behaviour such as littering leads to lower scores. In order to fulfil \n",
            "the score’s aim, i.e., to encourage good behaviour and citizenship, bad scores\n",
            "\n",
            "\n",
            "Content:\n",
            "humans as workers, the idea of a universal basic income will be put back on the \n",
            "table. This would trigger a series of fundamental philosophical but also reli -\n",
            "gious debates: questions such as the purpose of life, how to feel useful and what \n",
            "to strive for, are some issues for which society would have to find answers. Eth-\n",
            "ics and education will play an important role in order to tackle these societal \n",
            "challenges and questions (Kaplan 2020a). \n",
            "About Democracy and Freedom\n",
            "Finally, AI progress could represent nothing less than a danger to peace and \n",
            "democracy (Kaplan 2020b). There are at least two ways in which artificial  \n",
            "intelligence might constitute a threat to democracy and its mechanisms,  \n",
            "endangering the peaceful coexistence of humans and machines: supervision \n",
            "and manipulation.\n",
            "Using the example of China, we will provide an illustration as to how far the \n",
            "possibilities of artificial intelligence reach with respect to control and super -\n",
            "\n",
            "\n",
            "Content:\n",
            "poses, there is concern about dangerous and problematic uses of the technol -\n",
            "ogy, which has prompted a global conversation on the normative principles \n",
            "to which AI ought adhere, under the banner of ‘ AI ethics’ . Governments, cor-\n",
            "porations and NGOs throughout the world have generated their own sets of \n",
            "AI ethics principles. Questions and critiques arise about the content of these \n",
            "ethics principles, whether they are actually implemented, and their (legal) \n",
            "enforceability (Wagner 2018). Broader issues emerge about the power and \n",
            "privilege of the organisations, governments and individuals which are creat -\n",
            "ing and implementing AI and accompanying ethical principles. For example, \n",
            "Google has recently announced an ethics service (Simonite 2020), yet has been \n",
            "mired in ethics controversies from violating privacy law (Finley 2019), work -\n",
            "ing on controversial military projects (Crofts and van Rijswijk 2020) and dis -\n",
            "\n",
            "\n",
            "Content:\n",
            "have instead overwhelmingly championed liberal frameworks based on citizen \n",
            "and consumer rights. If we are to contend with AI in relation to the advance -\n",
            "ment of a more just society, then such frameworks are insufficient. \n",
            "Governing AI \n",
            "The advent of AI has sustained much discussion about what is actually at stake \n",
            "with the growing datafication of social life. Whilst there is widespread recogni-\n",
            "tion that the rapid development and deployment of data-centric technologies \n",
            "has significant transformative implications, the question as to what these are \n",
            "and how they should be addressed is still up for grabs. Gangadharan (2019) has \n",
            "provided a useful overview of different frameworks for data governance that \n",
            "highlights some of the dominant ways in which AI and data-driven systems in \n",
            "general have been approached in governance debates, including privacy policy, \n",
            "data protection, ethics, fairness-in-design and human rights. Elaborating on\n",
            "\n",
            "\n",
            "Content:\n",
            "Artificial Intelligence (AI): When Humans and Machines Might Have to Coexist   27\n",
            "conscience, are more likely to be biased, essentially because the data on which \n",
            "they were trained was biased. A study by Wilson, Hoffman and Morgenstern \n",
            "(2020) illustrates that several decision-support systems applied by judges \n",
            "may be racially biased (as a result of past rulings); and self-driving cars better \n",
            "detected lighter skin than darker tones, since their algorithm was trained using \n",
            "pictures among which were few people of colour.\n",
            "Regulation and guidance is definitely needed in order to avoid such bias, to \n",
            "establish a good foundation for machine < > human collaboration. The devel -\n",
            "opment of specific requirements with respect to the testing and training of AI is  \n",
            "likely the preferred approach, as opposed to regulating artificial intelligence \n",
            "itself. In addition, we could require AI warranties, consistent with safety test -\n",
            "\n",
            "\n",
            "Content:\n",
            "214 AI for Everyone?\n",
            "their secrecy (Coeckelbergh 2020, Ch. 8). It is often the case that we do  \n",
            "not understand how algorithms function and interrelate, what exactly their \n",
            "operation encompasses, what impact they have on our lives and under what \n",
            "conditions this happens. This is why algorithms can lead to results and con -\n",
            "sequences that might not be intended in the first place and sometimes cannot \n",
            "even be adequately explained.\n",
            "There have been numerous cases of encoded  biases in algorithms such as \n",
            "racist profiling or sexism (Bridle 2018, 142), which were a consequence of \n",
            "comparable biases historically existing in society. The poet Joy Buolamwini, for \n",
            "example, criticized them in a project AI, Ain’t I A Woman (www.notflawless.ai),  \n",
            "which focused on grave failures of facial recognition when it came to black \n",
            "women. A myriad of such incidents demonstrates both that algorithms are far \n",
            "from neutral artefacts, a point I return to later, but also that even their designers\n",
            "\n",
            "\n",
            "Content:\n",
            "to an already skewed social landscape (Eubanks 2018). There’s more data about \n",
            "the poor and marginalised because they are already most surveilled, and they \n",
            "are most surveilled because our social systems already categorise them as trou-\n",
            "blesome. As a result, any unfairness that algorithms add to the mix will fall \n",
            "more heavily on those who are already struggling the most. However, it’s not \n",
            "only or even mainly data that shapes the politics of AI.\n",
            "Langdon Winner wrote about the way particular technologies appear to \n",
            "have an inherent compatibility with particular socio-political systems (Winner \n",
            "2020), so it’s fair to ask what feedback loops connect AI and the societies into \n",
            "which it has emerged. This attentiveness may help to bring neglected features  \n",
            "to the fore, to remind us of framings that are so pervasive they are usually \n",
            "ignored or to highlight new dynamics that are going to change more than just\n",
            "\n",
            "\n",
            "'''\n",
            "\n",
            "## GUIDELINES\n",
            "1. **Accuracy**:\n",
            "   - Only use the content in the `CONTEXT` section to answer.\n",
            "   - If the answer cannot be found, explicitly state: \"The provided context does not contain this information.\"\n",
            "   - Start explain AI  contribution to the society and civil liberties\n",
            "   - Follow by current correlation between AI and civil liberties\n",
            "   - Lastly explain AI legislation with civil liberties perspective.\n",
            "\n",
            "2. **Transparency**:\n",
            "   - Reference the book's name and page numbers when providing information.\n",
            "   - Do not speculate or provide opinions.\n",
            "\n",
            "3. **Clarity**:\n",
            "   - Use simple, professional, and concise language.\n",
            "   - Format your response in Markdown for readability.\n",
            "\n",
            "## TASK\n",
            "1. Answer the user's question **directly** if possible.\n",
            "2. Point the user to relevant parts of the documentation.\n",
            "3. Provide the response in the following format:\n",
            "\n",
            "## RESPONSE FORMAT\n",
            "'''\n",
            "# [Brief Title of the Answer]\n",
            "[Answer in simple, clear text.]\n",
            "\n",
            "**Source**:\n",
            "• [Book Title], Page(s): [...]\n",
            "'''\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "wLPAcchBxYZ5",
        "outputId": "976c7800-16ed-41fe-c4cf-58f60d3230d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To provide a comprehensive answer on how civil liberties can be compromised by AI, I will focus on several key aspects often discussed in critical perspectives of AI usage:\n",
            "\n",
            "1. **Surveillance**: AI technologies, especially those used in surveillance systems, can significantly compromise civil liberties. Facial recognition and other monitoring tools can lead to mass surveillance, where individuals are constantly monitored without their consent. This can infringe on the right to privacy and lead to a chilling effect on free speech and assembly as people may alter their behavior due to the awareness of being watched.\n",
            "\n",
            "2. **Bias and Discrimination**: AI systems often reflect and perpetuate existing biases found in the data they are trained on. This can result in discriminatory outcomes that affect people's civil rights, such as biased hiring algorithms or unfair law enforcement practices that disproportionately target minority communities. Such discrimination undermines equality before the law and can violate anti-discrimination laws.\n",
            "\n",
            "3. **Lack of Transparency**: Many AI systems operate as \"black boxes\" with decision-making processes that are not transparent or understandable to users or those affected by their outcomes. This lack of transparency can compromise accountability mechanisms essential for protecting civil liberties, as it becomes difficult to challenge decisions made by these systems.\n",
            "\n",
            "4. **Autonomy and Consent**: The deployment of AI in various sectors without adequate consent from individuals raises concerns about autonomy over personal data and decision-making processes. For instance, personal information might be used by AI systems without explicit permission, which violates principles of informed consent.\n",
            "\n",
            "5. **Due Process**: In scenarios where AI is used for decision-making purposes—such as credit scoring, job recruitment, or criminal sentencing—there is a risk that individuals may not have proper avenues to contest or appeal against these algorithmic decisions. This absence of due process protections can undermine justice and fair treatment.\n",
            "\n",
            "These areas highlight critical intersections between AI technology deployment and potential threats to civil liberties, emphasizing the need for careful consideration, regulation, and oversight when integrating AI into societal frameworks.\n"
          ]
        }
      ],
      "source": [
        "answer = completion.choices[0].message.content\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXVNXPwLxYaT"
      },
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png\" alt=\"NLP Gif\" style=\"width: 500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldybhlqKxYaT"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Cosine Similarity</h2>\n",
        "\n",
        "**Cosine similarity** is a metric used to measure the alignment or similarity between two vectors, calculated as the cosine of the angle between them. It is the **most common metric used in RAG pipelines** for vector retrieval.. It provides a scale from -1 to 1:\n",
        "\n",
        "- **-1**: Vectors are completely opposite.\n",
        "- **0**: Vectors are orthogonal (uncorrelated or unrelated).\n",
        "- **1**: Vectors are identical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1I1TNhxYaT"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg\" alt=\"NLP Gif\" style=\"width: 700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoEMdNgQxYaU"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Keyword Highlighting</h2>\n",
        "\n",
        "Highlighting important keywords helps users quickly understand the relevance of the retrieved text to their query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "nCXL9Cz1xYaV"
      },
      "outputs": [],
      "source": [
        "from termcolor import colored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwDyofY0xYaV"
      },
      "source": [
        "The `highlight_keywords` function is designed to highlight specific keywords within a given text. It replaces each keyword in the text with a highlighted version using the `colored` function from the `termcolor` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "9y3E0YWExYaV"
      },
      "outputs": [],
      "source": [
        "def highlight_keywords(text, keywords):\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, colored(keyword, 'green', attrs=['bold']))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice4: add your keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "i7SkWPpnxYaW",
        "outputId": "28e82563-edba-4b41-acad-ec27e5ba134f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snippet 1:\n",
            "never satisfy \u001b[1m\u001b[32mrights\u001b[0m-based or duty-based obligations to other \u001b[1m\u001b[32mhuman\u001b[0ms such as \n",
            "protection from persistent surveillance. \n",
            "However, AI for social good is ad hoc. That is to say, private individuals gen-\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "query_keywords = [\"civil\", \"liberties\", \"human\", \"rights\" ] # add your keywords\n",
        "for i, doc in enumerate(retrieved_docs[:1]):\n",
        "    snippet = doc.page_content[:200]\n",
        "    highlighted = highlight_keywords(snippet, query_keywords)\n",
        "    print(f\"Snippet {i+1}:\\n{highlighted}\\n{'-'*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhV_Jf_LxYaX"
      },
      "source": [
        "1. `query_keywords` is a list of keywords to be highlighted.\n",
        "2. The loop iterates over the first document in retrieved_docs.\n",
        "3. For each document, a snippet of the first 200 characters is extracted.\n",
        "4. The highlight_keywords function is called to highlight the keywords in the snippet.\n",
        "5. The highlighted snippet is printed along with a separator line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBRKysAvxYaX"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Bonus</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj25lCybxYaX"
      },
      "source": [
        "**Try loading one of your own PDF books and go through the steps again to explore how the pipeline works with your content**:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
